---
Layout: post
mathjax: true
comments: true
title:  "Taking half a derivative"
categories: [Mathematics]
date:  2021-03-13
---

**March 13, 2021.** *Can you take half a derivative? It turns out the
  answer is yes, and there are two apparently different ways to define
  it. By playing around with the Gamma function, I show one implies
  the other!*

#### Introduction

In calculus, the regular derivative is defined as the local gradient
of a function:

$$
\frac{d}{dx} f(x) = \lim_{h\to 0}\frac{\Delta f}{\Delta x} = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}.
$$

We will abbreviate this as $f' = f^{(1)} = Df$, understanding that $f$ is a function
of $x$ and $D$ differentiates with respect to $x$ (though we can add a
subscript $D_x$ if we want to be very careful).
We can always differentiate again, and again, and in fact as many
times as we want. Using our new notation, we can write the $n$th
derivative $f^{(n)}$ as

$$
f^{(n)} = D (D \cdots (Df)) = D^n f.
$$

This is well-defined as long as $n$ is a whole number.
But what if we could consider other types of derivatives, say half a
derivative? Let's call this $D^{1/2} = \sqrt{D}$. In the same way that
applying two ordinary derivatives gives the second derivative
$f^{(2)}$, it seems reasonable to hope that two half derivatives give
a full derivative:

$$
f' = \sqrt{D} \sqrt{D}f = Df \quad \Longrightarrow \quad \sqrt{D}
\cdot \sqrt{D} = D.
$$

What could half a derivative look like?

#### To be continued

The easiest way to go about this to use a trick called *analytic
continuation*.
This has a precise meaning in complex analysis, and we're going to do
something similar in spirit, but not truly rigorous.
The basic plan is just be to find nice, specific functions we can
differentiate $n$ times, and get in an answer in terms of $n$.
We'll define the *fractional derivative* $D^\alpha$ acting on these
functions by simply replacing $n$ with $\alpha$.
A sanity check will be that, for general $\alpha, \beta$, the
fractional derivatives obey

$$
D^\alpha \cdot D^\beta = D^{\alpha+\beta},
$$

so, e.g., two half-derivatives give a full derivative,
$\sqrt{D}\cdot \sqrt{D} = D$.
We call this the *multiplicative* property.
There are two issues with this approach.
First, how do we extend the definition to general functions?
And second, are the definitions for different functions in agreement?
We will answer both questions below, but for the moment, we will
simply push ahead and do our continuations.
Our first nice function is the exponential $e^{\omega x}$.
Differentiating simply pulls down a factor of $\omega$ each time, so

$$
D^n e^{\omega x} = \omega^n e^{\omega x}.
$$

It's very clear, then, how to define the fractional derivative acting
on this:

$$
D^\alpha e^{\omega x} = \omega^\alpha e^{\omega x}.
$$

Great! We can easily check the multiplicative property, assuming that
constants pass through the derivatives:

$$
D^\alpha D^\beta e^{\omega x} = \omega^\alpha D^\beta e^{\omega x} =
\omega^{\alpha + \beta} e^{\omega x} = D^{\alpha+\beta}e^{\omega x}.
$$

Now, you might think this is useless because we can only
take fractional derivatives of exponential functions.
But at this point, we introduce another assumption, namely that the
fractional derivatives are *linear*:

$$
D^\alpha (\lambda_1 f_1 + \lambda_2 f_2) = \lambda_1 D^\alpha f_1 + \lambda_2 D^\alpha f_2.
$$

where $f_1, f_2$ are functions and $\lambda_1, \lambda_2$ are constants.
In fact, let's suppose this linearity applies to an *infinite*
collection of functions $f$ and constants $\lambda$, arranged into an integral

$$
f(x) = \int_{-\infty}^\infty d\omega \, \lambda(\omega) e^{\omega x}.
$$

Then by linearity,

$$
D^\alpha f(x) = \int_{-\infty}^\infty d\omega \, \lambda(\omega) D^\alpha
e^{\omega x} = \int_{-\infty}^\infty d\omega \, \lambda (\omega)
\omega^\alpha e^{\omega x}. \tag{1} \label{exp}
$$

Functions which can be written this way are said to have a *Fourier
representation*, with the function $ \lambda (\omega)$ the *Fourier
transform*. Most functions have one!
But let's do a very simple example.
What is half a derivative of the familiar sine function?
We can write this in terms of exponentials as

$$
\sin(x) = \frac{1}{2i}(e^{ix} - e^{-ix}).
$$

Take a half-derivative using linearity gives

$$
\sqrt{D} \sin(x) = \frac{1}{2i}(\sqrt{D} e^{ix} - \sqrt{D} e^{-ix}) = \frac{1}{2i}(\sqrt{i} e^{ix} - \sqrt{-i} e^{-ix}).
$$

This is evidently not a real function!
It should also be clear there is some ambiguity about
which roots we choose.
This means that, even at this point, there are different ways to
define the fractional derivative.
In general this ambiguity is harmless, and we just take the principal
values (with arguments between $-\pi$ and $\pi$) but below we will need
to take a bit more care.
Note that the same techniques let us do crazy things like take $i$
derivatives! We just set $\alpha = i$, so the $i$th derivative of sine is

$$
D^i \sin(x) = \frac{1}{2i}(i^i e^{ix} - (-i)^i e^{-ix}) =
\frac{1}{2i}(e^{-\pi/4 + ix} - e^{+\pi/4 - ix}),
$$

since the principal values are

$$
i^i = e^{i (i \pi/4)} = e^{-\pi/4}, \quad (-i)^i = e^{i (-i \pi/4)} = e^{\pi/4}.
$$

#### Fractorials

But this isn't the only way to approach fractional derivatives. The
first function we encounter is usually the identity function $f(x) =
x$.
From there, we build up to polynomials $x^m$, and then arbitrary
powers $x^s$.
The derivative of a power has a very simple form:

$$
D x^s = s x^{s-1}.
$$

If we differentiate again, we bring down a factor of $s - 1$ and
reduced the index again. And so on. This leads to the expression for
$n$ derivatives:

$$
D^n x^s = s(s- 1) \cdots (s - n + 1) x^{s-n}.
$$

So far, this doesn't look nice.
But let's assume for a moment $s$ is an integer.
Then we can write

$$
s(s- 1) \cdots (s - n + 1) = \frac{s(s - 1) (s-2) \cdots 1}{(s -
n)(s-n - 1) \cdots 1} = \frac{s!}{(s -n)!},
$$

where we have introduced the factorial $s!$.
Thus, we can write

$$
D^n x^s = \frac{s!}{(s -n)!} x^{s-n}.
$$

To analytically continue this, we need a beautiful function called the
Gamma function $\Gamma$.
This gives us the factorial function at (shifted) integer values,

$$
\Gamma(k + 1) = k!
$$

but is defined elsewhere as well. I like to think of it as a
"fractorial" because it can take on fractional values. In addition to
giving us this delightfully bad pun, we can use the Gamma function to write

$$
D^n x^s = \frac{\Gamma(s + 1)}{\Gamma(s -n + 1)} x^{s-n},
$$

and immediately continue to the fractional derivative:

$$
D^\alpha x^s = \frac{\Gamma(s + 1)}{\Gamma(s -\alpha + 1)}
x^{s-\alpha}. \tag{2} \label{power}
$$

Too easy! Once again, we can check the multiplicative property:

$$
\begin{align*}
D^\alpha D^\beta x^s & = \frac{\Gamma(s + 1)}{\Gamma(s -\beta + 1)}
D^\alpha x^{s-\beta} \\
& = \frac{\Gamma(s + 1)}{\Gamma(s -\beta + 1)}
\cdot \frac{\Gamma(s - \beta + 1)}{\Gamma(s -\alpha - \beta + 1)}
x^{s-\beta - \alpha} \\
& = \frac{\Gamma(s + 1)}{\Gamma(s -\alpha -\beta + 1)}x^{s-\beta -
\alpha}  = D^{\alpha+\beta} x^s.
\end{align*}
$$

So this gives us another, evidently different way to define fractional
derivatives. It will apply to any sum or integral of powers of
$x$, for instance, infinite polynomials called *power series*, and
their close cousins the *Laurent series*, which include reciprocal powers:

$$
\sum_{k = 0}^\infty a_k x^k, \quad \sum_{k = -\infty}^\infty b_k x^k.
$$

These cover a lot of ground, but there is an even more general object
called the *Mellin transform*, analogous to the Fourier transform. But
we won't go there.
Instead, let's do another simple example.
One of the interesting properties of the Gamma function is that it
blows up to (minus) infinity for nonpositive integers:

$$
\Gamma(-n) = -\infty, \quad n = 0, 1, 2, \ldots.
$$

This is actually essential to get sensible answers!
For instance, let's take the derivative of a constant, $1 = x^0$.
Then according to our definition,

$$
D x^0 = \frac{\Gamma(0 + 1)}{\Gamma(0 -1 + 1)} x^{0 - 1} =
\frac{\Gamma(1)}{\Gamma(0)} x^{- 1} = 0,
$$

since the $\Gamma(0)$ in the denominator makes the whole thing vanish.
Incidentally, these infinities can sometimes *cancel* to produce
sensible results.
For instance, if we take a derivative of $1/x$, we should get
$-1/x^2$. If we plug in our formula, we have

$$
D x^{-1} = \frac{\Gamma(-1 + 1)}{\Gamma(-1 -1 + 1)} x^{-1 - 1} =
\frac{\Gamma(0)}{\Gamma(-1)} x^{-2}.
$$

Both the numerator and the denominator blow up, which should make us
queasy. But there is a trick here to get the right answer with only
mild violence to mathematical rigour. It turns out that for any $z$,
the Gamma function obeys the *functional equation*

$$
\Gamma(1 + z) = z\Gamma(z).
$$

Since $\Gamma(k + 1) = k!$, this gives the usual relation for factorials,

$$
k! = \Gamma(k + 1) = k\Gamma(k) = k \cdot (k - 1)!,
$$

but also the sneaky $\Gamma(0) = (-1)\Gamma(-1)$. Both $\Gamma(0)$ and
$\Gamma(-1)$ blow up of course, but in the derivative of $1/x$, the
$\Gamma(-1)$ terms cancel, leaving $(-1)x^{-2} = -1/x^2$ as required.

#### Gamma and tongs

Now we come to the crucial question: do these two methods agree?
Rather than give a fully general, rigorous proof, I'm going to content
myself with showing that the exponential definition (which I regard as
simpler) gives rise to the power definition.
At this point, it will become necessary to define the Gamma function:

$$
\Gamma(s) = \int_{0}^\infty dt\, t^{s-1} e^{-t}.
$$

This isn't looking particularly useful yet. But let's make the sneaky
change of variables $t = \omega x$.
This gives

$$
\Gamma(s) = x^{s} \int_{0}^\infty d\omega\, \omega^{s-1} e^{-\omega
x}.
$$

If we change $s \to -s$, and rearrange, we get a formula for $x^s$
in terms of exponentials:

$$
x^{s} = \frac{1}{\Gamma(-s)}\int_{0}^\infty d\omega\, \omega^{-(1+ s)}
e^{-\omega x}. \tag{3} \label{gamma}
$$

Great! Now we just go ahead and use rule (\ref{exp}), with the hope we
will get rule (\ref{power}).
As usual, we proceed using linearity:

$$
\begin{align*}
D^\alpha x^{s} & = \frac{1}{\Gamma(-s)}\int_{0}^\infty d\omega\,
\omega^{-(1+ s)} D^\alpha e^{-\omega x} \\
& = \frac{1}{\Gamma(-s)}\int_{0}^\infty d\omega\,
\omega^{-(1+ s)} (-\omega)^\alpha e^{-\omega x} \\
& = (-1)^\alpha\frac{1}{\Gamma(-s)}\int_{0}^\infty d\omega\,
\omega^{-(1+ s - \alpha)} e^{-\omega x} \\
& = (-1)^\alpha\frac{\Gamma[-(s-\alpha)]}{\Gamma(-s)} x^{s-\alpha},
\end{align*}
$$

where on the last line we used (\ref{gamma}), but with $s
-\alpha$ instead of $s$.
This isn't quite what we want.
To make progress, we're going to use something called the *reflection
formula* for the Gamma function (derived [here](https://hapax.github.io/mathematics/zeta/)
for instance):

$$
\Gamma(z) \Gamma(1 - z) = \frac{\pi}{\sin(\pi z)}.
$$

We can apply this to both $\Gamma(-s)$ and $\Gamma[-(s-\alpha)]$ to
get

$$
\begin{align*}
D^\alpha x^{s} & = (-1)^\alpha \frac{\sin(\pi
s)}{\sin[\pi(s-\alpha)]}\cdot \frac{\Gamma(s+1)}{\Gamma(s-\alpha + 1)} x^{s-\alpha}.
\end{align*}
$$

This is almost (\ref{power}), the thing we were after!
But there is this strange factor with sines out the front.
Recall the definition of sine in terms of complex exponentials.
This lets us write the funny factor as

$$
(-1)^\alpha \frac{\sin(\pi s)}{\sin[\pi(s-\alpha)]} = \frac{e^{\pi i
s} - e^{-\pi i s}}{(-1)^\alpha e^{\pi i (s-\alpha)} - (-1)^\alpha e^{-\pi i (s-\alpha)}}.
$$

It would be magical if that $(-1)^\alpha$ could somehow behave
differently and cancel the $\alpha$ terms floating around, right?
Well, turns out it does!
We can write $-1 = e^{\pm \pi i}$, and hence

$$
(-1)^\alpha = e^{\pm \pi i \alpha}.
$$

I won't spell out the details, but if you look at the
[proof](https://hapax.github.io/mathematics/zeta/) of the reflection
formula, the two different terms in the sine arise from parts of a
integration contour which lie in almost the same place, but we take
roots in different ways.
In particular, evaluating $(-1)^\alpha$ gives different factors
$e^{\pm \pi i \alpha}$ for the terms they multiply, and perfectly
cancels them, so that our funny factor is actually unity:

$$
\frac{e^{\pi i
s} - e^{-\pi i s}}{(-1)^\alpha e^{\pi i (s-\alpha)} - (-1)^\alpha
e^{-\pi i (s-\alpha)}} = \frac{e^{\pi i
s} - e^{-\pi i s}}{e^{\pi i \alpha} e^{\pi i (s-\alpha)} - e^{-\pi i \alpha}
e^{-\pi i (s-\alpha)}} = \frac{e^{\pi i
s} - e^{-\pi i s}}{e^{\pi i s} - e^{-\pi i s}} = 1.
$$

Thus, our exponential rule actually
reproduces the rule from fractorials!
Now, to be clear, fractional derivatives are a big and mathematically
heavy topic, and I've only skimmed the surface.
But it's neat that the two simplest approaches agree.

#### Acknowledgments

Thanks to J.A. for inviting a discussion of fractional derivatives.

<!-- Our exponential definition yields an *antiderivative* operator:
$$
D^{-1} e^{\omega x} = \frac{1}{\omega}e^{\omega x}.
$$
This is the usual antiderivative, except without the constant. -->
